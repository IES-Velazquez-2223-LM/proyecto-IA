<!DOCTYPE html>
<html lang="es">

<head>
    <meta name="robots" content="index, follow">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="En esta web repasamos las últimas noticias sobre IA">
    <meta name="author" content="Marco Blanco, Jose Luis Pino">
    <meta name="copyright" content="MachineLearners.com">
    <meta name="keywords" content="IA, Machine Learning, Inteligencia Artificial">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="./icon/robot_icon.png">
    <title>The Machine Learners</title>
</head>

<body>
    <h1>La gran batalla por el liderazgo en la Inteligencia Artificial</h1>
    <p>OpenAI es una fundación originada por Elon Musk y ahora muy apoyada por Microsoft, entre otros, que promueve la
        investigación y desarrollo de redes neuronales de última generación con alto impacto en la sociedad. Aunque
        OpenIA es sin ánimo de lucro, es obvio que todos los que participan vislumbran unas enormes posibilidades de
        negocio.</p>
    <p>Su mas reciente logro, ChatGPT basado en GPT-3.5, acapara todas las portadas y todo el protagonismo de la
        Inteligencia Artificial en las últimas semanas.</p>
    <p>este algoritmo, que permite responder en modo conversacional a cualquier pregunta con respuestas acordes con la
        lógica de los datos e idiomas con los que se ha entrenado, provoca furor y sorprende por la frescura y
        espontaneidad de sus respuestas.</p>
    <p>Sus derivaciones con Dall E2 llevan a plantear imágenes realistas a partir de descripciones en texto lo cual
        permite idear cualquier tipo de imagen o, por ejemplo, completar imágenes de las que sólo tenemos una
        parcialidad, en el caso de pinturas o cuadros.</p>
    <h2>Algoritmos auto supervisados tipo BERT</h2>
    <p>Primero de todo, el surgimiento de algoritmos auto supervisados tipo BERT (bidirectional encoder representations
        from transformers) que surgen de Google en 2018 para interpretar mejor nuestras búsquedas y superar el 85% de
        respuesta a las consultas realizadas. Recordemos que Google lleva indexando la web desde hace más de dos
        décadas, y de esto sabe un poco. La idea de Google con BERT es que interprete contextos, no solo palabras de
        búsqueda, para así afinar más. Hay que tener en cuenta que cada día se hacen un 15% de búsquedas nuevas, que
        nunca se habían visto.</p>
    <p>Desde que BERT nace en Google AI Research provoca una mejora en casi dos decenas de problemas que surgían con el
        procesamiento de lenguaje natural. El secreto de BERT viene del principio acuñado en los 60 por Rupert Firth,
        quien decía que se puede conocer el significado de una palabra solo por las que tiene alrededor. En su
        entrenamiento, la paternidad de Google hace que el algoritmo salga potenciado por el increíble entrenamiento de
        todo su patrimonio lingüístico, lo que le hace poderoso e imbatible.</p>
    <p>Usa “Transformers”, que es un modelo de red neuronal que data de 2017 y que supera a las redes neuronales
        recurrentes que presentaban problemas con el texto y el lenguaje, sobre todo con párrafos largos. Los
        Transformers miden los pesos de cada palabra dentro de cada oración y de su relación, por lo que llega un
        momento que es capaz de predecir la siguiente palabra. Estos sí pueden entrenarse con párrafos largos y son
        ideales para traducciones. GPT-3.5 por ejemplo se entrenó con 45 TB de texto de toda la web, lo cual le da una
        gran potencia, y por lo que sabemos, su próxima versión GPT-4 será mucho mas potente.</p>
    <p>En definitiva, BERT es el modelo más “popular” de todos los Transformers y constituye un modelo entrenado en sí
        mismo por investigadores de Google con un corpus de texto masivo. Pero, a raíz de BERT han surgido derivaciones
        como RoBERTa que es usado por Facebook, DistilBERT o XLNet, que mejoran rendimiento y capacidad de cómputo y
        usan diferentes entrenamientos. En realidad, todos ellos resuelven las mismas tareas:</p>
    <ul>
        <li>Resumen de textos.</li>
        <li>Respuestas a preguntas.</li>
        <li>Clasificación y resolución de entidades con nombre.</li>
        <li>Búsqueda de textos similares.</li>
        <li>Detección de fakes o mensajes malignos.</li>
        <li>Entendimiento conversacional con el usuario.</li>
    </ul>
</body>

</html>